# Machine Learning Engineer Nanodegree
## Capstone Proposal
Junil Park  
September 6th, 2020

## Proposal

### Domain Background
When determining the structure of CNN, hyperparameters such as kernel size, stride, and number of channels must be determined. These parameters not only determine the overall structure of the CNN, but also directly affect the performance such as learning time and accuracy. Therefore, in order to obtain the expected result without falling into the overfitting problem, the optimization work for the hyperparameter must be preceded. In this regard, various papers and research results can be confirmed, but it is common that there are still no specific rules for the correlation and performance impact of each layer structure for image classification and drop-out parameters. When setting the hyperparameter of, after deciding a lot of parts according to experience or the designer's intuition, it is a situation that requires a lot of trial and error. Looking at the CNN structure that actually shows good performance, in the LeNet-5 structure, the size of all kernels was set equal to 5, but Alexnet set the size of the kernel to 11, 5, 3, and ZFNet set the size of the kernel to 7, 5, 3. As a result, it can be seen that the hyperparameter was set according to the designer's intuition.

### Problem Statement
In this report, a simple structured CNN is implemented using Python-based Tensorflow, and CIFAR-10 image set is trained on the neural network according to changes in dropout ratio, batch size, CNN layer structure, and fully connected layer structure. I tried to observe the change in CNN's image learning performance and interpret its meaning. Basically, it is impossible for humans to grasp the meaning of the learning result through the neural network in the form of a black box or to grasp the final result through any calculation or interpretation. Therefore, after setting some hyperparameters as fixed constants, by changing the values of each parameter to be observed for optimization, we examine the results of how the final accuracy changes in a way that suggests how the parameter affects the neural net. It was attempted to observe and interpret the phenomenon using the method.

### Datasets and Inputs
CIFAR-10 and CIFAR-100 are sub-sets of 80,000,000 small image data sets, which were collected by Alex Krizhevsky, Vinod Nair and Geoffrey Hinton. The CIFAR-10 Dataset contains a total of 60000 32x32 color images of 10 classes and consists of 6000 images per class. 50000 training images and 10000 test images are included. This dataset is divided into 5 training batches and 1 test batch, and there are 10000 images in each batch. The test batch contains exactly 1000 images randomly selected for each class. The training batch contains the remaining images in random order, but some training batches may contain more images than other classes. Here is a sample of 10 random images of each, as well as the class of the data set.
Each class is completely mutually exclusive, and there is no overlap between car and truck labeling. "Automotive" includes sedans, SUVs, etc. "Truck" includes only large trucks, not pickup trucks. The implemented python code will be used to download the CIFAR-10 data set for Python. It's necessary to consider the logic that minimizes the memory consumption of the computer as the data set is batch processed.

### Solution Statement
In this experiment, the structure of the hyperparameter such as the channel and stride of the CNN abstraction step is fixed to a specific value, and the batch size, the structure of the CNN layer, the structure of the fully connected layer, and the dropout ratio are set as variables, based on the training accuracy of the neural network. Simulation is carried out by updating each hyperparameter variables and the accuracy will be evaluated for each cases.
In this report, rather than a viewpoint for optimizing the CNN structure and hyperparameters targeted by existing studies, the values ​​for each hyperparameter presented as a study target are gradually changed to a grid search method within a certain range, respectively, at each step. By observing the change in neural network accuracy, we intend to use it as the basis for designing and optimizing artificial neural networks for new datasets and research subjects in the future.

### Benchmark Model
When performing neural network training through CNN, the simplest way to determine the structure of a CNN is the cross-validation [1] technique, which selects the structure with the best performance by performing a neural network of various structures. However, cross-validation can be said to be a limited method because the best performance is guaranteed only within a few artificially selected structures among the numerous conditions that can constitute a CNN. The most common strategy used for hyperparameter optimization is the so-called grid search method, in which a specific range is set for each parameter and the number of all cases that can be combined within it is substituted. The disadvantage of this grid search method is that the time and effort for calculation increase exponentially according to the number of parameters to be optimized and the desired search level [2]. Recently, there is a research result that the random search [3] method, which performs randomly selecting hyperparameters within the desired search range, shows better results in finding a model with computational cost or accuracy than grid search. However, when setting the optimal CNN structure, the grid search or random search method is not a method of referencing the previous evaluation results when evaluating a new hyperparameter set. Recently, when performing hyperparameter optimization, Bayesian optimization is also widely used [4, 5, 6], and Bayesian optimization is a method of generating a probability model M based on the previous evaluation result of the objective function f. In order to implement Bayesian optimization, the Gaussian process model (probability model M) and Spearmint [4] technique using the Sequential Model-based Algorithm Configuration (SMAC) based on the random forest of the Gaussian process are most often used. According to the research results of [7], it is reported that Bayesian optimization has poor predictive performance and high computational cost in a CNN structure involving high-dimensional and complex hyperparameters. In the study of [4], a Bayesian optimization technique based on the Gaussian process was used to optimize hyperparameters such as the learning rate of CNN, the epoch, and the initial weight of the convolution and fully-connected layers. Many of the hyperparameters in this study have continuous values ​​and are affected by the normalization of the data, but have little correlation with the structure of the CNN itself. [8, 9, 10] also conducted optimization studies on the continuous hyperparameter values ​​of deep neural networks in a similar manner, and in [10, 11, 12, 13, 14, 15] studies, unlike previous studies, Applied techniques that can automatically update hyperparameter values ​​are proposed, such as adjusting the learning rate or reducing the weight value to improve the execution speed of backpropagation. In addition, early stopping [16, 17] is sometimes used when the accuracy of the artificial neural network is no longer improved as learning progresses or the error rate increases as the epoch for learning progresses. [18] In the paper, an effective technique that can be used to initialize the weights of the convolution layer and the fully-connected layers is proposed, and the Evolutionary algorithm is the most widely used in automating the configuration of the learning algorithm structure of artificial neural networks such as CNN. It can be called an algorithm. [19] The genetic algorithm used in the study is used to optimize the filter size and number of convolution layers. It has a structure consisting of three convolution layers and one fully-connected layer, and hyperparameters such as layer depth and pooling size are optimized. The accuracy is not as high as 75% because it is not in the state. The genetic algorithm has a disadvantage that the cost for computation is quite high, because all cases (population) involved in the genetic algorithm are composed of individual CNN objects, and each object needs to be trained, corrected, and verified every time. In the study of [20], optimization studies were conducted only on fully-connected layers of artificial neural networks, but a reqularization technique is used to automatically set the number of units of the corresponding fully-connected layer. In recent years, attention has been focused on the structural design-related parts of deep learning, and [21] applied reinforcement learning and RNN to the study to search for the optimal structure of neural networks, showing quite impressive results. In [22], it was proposed to use Evolution of Augmenting Topologies (NEAT) based on CoDeepNEAT to determine the shape of each layer of neural network and its hyperparameters. In [23], a genetic algorithm is used to design a complex CNN structure, and each of the above experiments used a method of simultaneously performing more than 250 variance tasks to find the optimal structure. Reinforcement learning based on Q-learning [24] was utilized as a method to search for an optimal structure for only one layer at a time, and the depth of the corresponding layer uses a value preset by the researcher.
However, the results obtained through reinforcement learning were tasks requiring considerable system resources and long hours of effort. [25] In the study, we used a method to visualize and monitor the optimization of neural network structures, and the speed was improved by reducing the stride and kernel size of the first layer of convolution. However, the problems inherent in the CNN structure used in the study or issues in progress had the disadvantage that a researcher with specialized knowledge must directly analyze it. In addition, humans had to be directly involved in the selection and selection of a new CNN structure. Most of the existing hyperparameter optimization or neural network model selection is a method of verifying various models in order to maximize the accuracy of the validation set. [26] In the study, deconvnet was proposed, which visualizes images learned through a neural network to measure and monitor the accuracy, thereby presenting a new method to reduce the error rate of CNN. This is a method that can be automatically set through. However, there is a disadvantage in that it cannot be applied to datasets other than image data because optimization is performed through visualization of the learned image.

### Evaluation Metrics
Evaluation of CNN performance changes according to each hyperparameter changes as below
#### dropout-ratio change (10 ~ 80%) and performence testing for each cases
#### batch size change (128 ~ 2048) and performence testing for each cases
#### convolutional layer change (2 ~ 6) and performence testing for each cases
#### fully connected layer change(1 ~ 5) and performence testing for each cases

### Project Design
Various hyperparameters such as the size of the kernel constituting the convolutional neural network (CNN), the number of channels, the stride, the number of convolution and max pooling layers, the number of fully connected layers, the number of epochs, the batch size, and the dropout ratio, etc. It has a decisive influence on not only determining but also extracting features. In this report, in addition to research on performance optimization based on kernel size, number of channels and stride structure, which have been proven in existing CNN-related papers, changes in CNN structure, max pooling size, dropout ratio, and fully connected network structure in the abstraction stage of CNN. The purpose of this report is to examine how it affects the final performance of this convolutional neural network. After basic configuration of the CNN, kernel, channel, and stride hyperparameter to have a certain performance through the implementation of a simple structure, CNN structure, max pooling size, dropout ratio, and fully connected layer to be observed are set as observation variables to learn. The trend and correlation of test performance changes were observed. The CNN model used in this paper is trained and tested using the CIFAR-10 Dataset.

### References
[1] Kohavi, R. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the 14th International Joint Conference on Artiﬁcial Intelligence, Montreal, QC, Canada, 20–25 August 1995; pp. 1137–1143.
[2] Schaer, R.; Müller, H.; Depeursinge, A. Optimized distributed hyperparameter search and simulation for lung texture classiﬁcation in CT using hadoop. J. Imaging 2016, 2, 19. 
[3] Bergstra, J.; Bengio, Y. Random search for hyper-parameter optimization. J. Mach. Learn. Res. 2012, 13, 281–305.
[4] Snoek, J.; Larochelle, H.; Adams, R.P. Practical bayesian optimization of machine learning algorithms. In Proceedings of the 25th International Conference on Neural Information Processing System, Lake Tahoe, NV, USA, 3–6 December 2012; pp. 2951–2959.  
[5] Hutter, F.; Hoos, H.H.; Leyton-Brown, K. Sequential model-based optimization for general algorithm conﬁguration. In Proceedings of the 5th International Conference on Learning and Intelligent Optimization, Rome, Italy, 17–21 January 2011; pp. 507–523. 
[6] Murray,I.;Adams, R.P.Slice sampling covariance hyperparameters of latent gaussian models. InProceedings of the 24th Annual Conference on Neural Information Processing Systems, Vancouver, BC, Canada, 6–9 December 2010; pp. 1723–1731. 
[7] Gelbart, M.A. 2015. Constrained Bayesian Optimization and Applications. Ph.D. Thesis, Harvard University, Cambridge, MA, USA, 2015. 
[8] Li, L.; Jamieson, K.; DeSalvo, G.; Rostamizadeh, A.; Talwalkar, A. Hyperband: A novel bandit-based approach to hyperparameter optimization. arXiv 2016, arXiv:1603.06560. 
[9] Loshchilov, I.; Hutter, F. CMA-ES for hyperparameter optimization of deep neural networks. arXiv 2016, arXiv:1604.07269. 
[10] Luketina, J.; Berglund, M.; Greff, K.; Raiko, C.T. Scalable gradient-based tuning of continuous regularization hyperparameters. arXiv 2015, arXiv:1511.06727. 
[11] Chan, L.-W.; Fallside, F. An adaptive training algorithm for back propagation networks. Comput. SpeechLang. 1987, 2, 205–218.
[12] Larsen, J.; Svarer, C.; Andersen, L.N.; Hansen, L.K. Adaptive Regularization in Neural Network Modeling. In Neural Networks: Tricks of the Trade; Springer: Berlin, Germany, 1998; pp. 113–132. 
[13] Pedregosa, F. Hyperparameter optimization with approximate gradient. arXiv 2016, arXiv:1602.02355. 
[14] Yu, C.; Liu, B. A backpropagation algorithm with adaptive learning rate and momentum coefﬁcient. In Proceedings of the 2002 International Joint Conference on Neural Networks, Piscataway, NJ, USA, 12–17 May 2002; pp. 1218–1223. 
[15] Zeiler, M.D. Adadelta: An adaptive learning rate method. arXiv 2012, arXiv:1212.5701.
[16] Caruana, R.; Lawrence, S.; Giles, L. Overﬁtting in neural nets: Backpropagation, conjugate gradient, and early stopping. In Proceedings of the 2001 Neural Information Processing Systems Conference, Vancouver, BC, Canada, 3–8 December 2001; pp. 402–408. 
[17] Graves, A.; Mohamed, A.; Hinton, G. Speech recognition with deep recurrent neural networks. In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, BC, Canada, 26–31 May 2013; pp. 6645–6649. 
[18] Glorot, X.; Bengio, Y. Understanding the difﬁculty of training deep feedforward neural networks. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, Sardinia, Italy, 13–15 May 2010; pp. 249–256. 
[19] Young, S.R.; Rose, D.C.; Karnowski, T.P.; Lim, S.-H.; Patton, R.M. Optimizing deep learning hyper-parameters through an evolutionary algorithm. In Proceedings of the Workshop on Machine Learning in High-Performance Computing Environments, Austin, TX, USA, 15–20 November 2015. 
[20] Kulkarni, P.; Zepeda, J.; Jurie, F.; Pérez, P.; Chevallier, L. Learning the structure of deep architectures using L1 regularization. In Proceedings of the British Machine Vision Conference, Swansea, UK, 7–10 September 2015; pp. 23.1–23.11. 
[21] Zoph, B.; Le, Q.V. Neural architecture search with reinforcement learning. arXiv 2016, arXiv:1611.01578. 
[22] Miikkulainen, R.; Liang, J.; Meyerson, E.; Rawal, A.; Fink, D.; Francon, O.; Raju, B.; Navruzyan, A.; Duffy, N.; Hodjat, B. Evolving deep neural networks. arXiv 2017, arXiv:1703.00548. 
[23] Real, E.; Moore, S.; Selle, A.; Saxena, S.; Suematsu, Y.L.; Le, Q.; Kurakin, A. Large-scale evolution of image classiﬁers. arXiv 2017, arXiv:1703.01041. 
[24] Baker, B.; Gupta, O.; Naik, N.; Raskar, R. Designing neural network architectures using reinforcement learning. arXiv 2016, arXiv:1611.02167. 
[25] Zeiler, M.D.; Fergus, R. Visualizing and understanding convolutional networks. In Proceedings of the European Conference on Computer Vision, Zurich, Switzerland, 6–12 September 2014; pp. 818–833.
[26] Saleh Albelwi; Mahmood. A Framework for Designing the Architectures of Deep Convolutional Neural Networks. Computer Science and Engineering Department, University of Bridgeport, Bridgeport, CT 06604, USA
