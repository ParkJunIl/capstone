# Machine Learning Engineer Nanodegree
## Capstone Proposal
Junil Park  
September 6st, 2020

## Proposal
Various hyperparameters such as the size of the kernel constituting the convolutional neural network (CNN), the number of channels, the stride, the number of convolution and max pooling layers, the number of fully connected layers, the number of epochs, the batch size, and the dropout ratio, etc. It has a decisive influence on not only determining but also extracting features. In this paper, in addition to research on performance optimization based on kernel size, number of channels and stride structure, which have been proven in existing CNN-related papers, changes in CNN structure, max pooling size, dropout ratio, and fully connected network structure in the abstraction stage of CNN. The purpose of this study was to examine how it affects the final performance of this convolutional neural network. After basic configuration of the CNN, kernel, channel, and stride hyperparameter to have a certain performance through the implementation of a simple structure, CNN structure, max pooling size, dropout ratio, and fully connected layer to be observed are set as observation variables to learn and learn. The trend and correlation of test performance changes were observed. The CNN model used in this paper was trained and tested using the CIFAR-10 Dataset.

### Domain Background
When determining the structure of CNN, hyperparameters such as kernel size, stride, and number of channels must be determined. These parameters not only determine the overall structure of the CNN, but also directly affect the performance such as learning time and accuracy. Therefore, in order to obtain the expected result without falling into the overfitting problem, the optimization work for the hyperparameter must be preceded. In this regard, various papers and research results can be confirmed, but it is common that there are still no specific rules for the correlation and performance impact of each layer structure for image classification and drop-out parameters. When setting the hyperparameter of, after deciding a lot of parts according to experience or the designer's intuition, it is a situation that requires a lot of trial and error. Looking at the CNN structure that actually shows good performance, in the LeNet-5 structure, the size of all kernels was set equal to 5, but Alexnet set the size of the kernel to 11, 5, 3, and ZFNet set the size of the kernel to 7, 5, 3. As a result, it can be seen that the hyperparameter was set according to the designer's intuition [2, 5, 6].

### Problem Statement
Various hyperparameters such as the size of the kernel constituting the convolutional neural network (CNN), the number of channels, the stride, the number of convolution and max pooling layers, the number of fully connected layers, the number of epochs, the batch size, and the dropout ratio, etc. It has a decisive influence on not only determining but also extracting features. In this paper, in addition to research on performance optimization based on kernel size, number of channels and stride structure, which have been proven in existing CNN-related papers, changes in CNN structure, max pooling size, dropout ratio, and fully connected network structure in the abstraction stage of CNN. The purpose of this study was to examine how it affects the final performance of this convolutional neural network. After basic configuration of the CNN, kernel, channel, and stride hyperparameter to have a certain performance through the implementation of a simple structure, CNN structure, max pooling size, dropout ratio, and fully connected layer to be observed are set as observation variables to learn and learn. The trend and correlation of test performance changes were observed. The CNN model used in this paper was trained and tested using the CIFAR-10 Dataset.

### Datasets and Inputs
CIFAR-10 and CIFAR-100 are sub-sets of 80,000,000 small image data sets, which were collected by Alex Krizhevsky, Vinod Nair and Geoffrey Hinton. The CIFAR-10 Dataset contains a total of 60000 32x32 color images of 10 classes and consists of 6000 images per class. 50000 training images and 10000 test images are included. This dataset is divided into 5 training batches and 1 test batch, and there are 10000 images in each batch. The test batch contains exactly 1000 images randomly selected for each class. The training batch contains the remaining images in random order, but some training batches may contain more images than other classes. Here is a sample of 10 random images of each, as well as the class of the data set.
Each class is completely mutually exclusive, and there is no overlap between car and truck labeling. "Automotive" includes sedans, SUVs, etc. "Truck" includes only large trucks, not pickup trucks. The following code was used to download the CIFAR-10 data set for Python. It is necessary to consider the logic that minimizes the memory consumption of the computer as the data set is batch processed.

### Solution Statement
In this experiment, the structure of the hyperparameter such as the channel and stride of the CNN abstraction step is fixed to a specific value, and the batch size, the structure of the CNN layer, the structure of the fully connected layer, and the dropout ratio are set as variables, based on the training accuracy of the neural network. Simulation was carried out by updating the variables.

### Benchmark Model
When performing neural network training through CNN, the simplest way to determine the structure of a CNN is the cross-validation [10] technique, which selects the structure with the best performance by performing a neural network of various structures. However, cross-validation can be said to be a limited method because the best performance is guaranteed only within a few artificially selected structures among the numerous conditions that can constitute a CNN. The most common strategy used for hyperparameter optimization is the so-called grid search method, in which a specific range is set for each parameter and the number of all cases that can be combined within it is substituted. The disadvantage of this grid search method is that the time and effort for calculation increase exponentially according to the number of parameters to be optimized and the desired search level [11]. Recently, there is a research result that the random search [12] method, which performs randomly selecting hyperparameters within the desired search range, shows better results in finding a model with computational cost or accuracy than grid search. However, when setting the optimal CNN structure, the grid search or random search method is not a method of referencing the previous evaluation results when evaluating a new hyperparameter set. Recently, when performing hyperparameter optimization, Bayesian optimization is also widely used [13, 14, 15], and Bayesian optimization is a method of generating a probability model M based on the previous evaluation result of the objective function f. In order to implement Bayesian optimization, the Gaussian process model (probability model M) and Spearmint [13] technique using the Sequential Model-based Algorithm Configuration (SMAC) based on the random forest of the Gaussian process are most often used. According to the research results of [16], it is reported that Bayesian optimization has poor predictive performance and high computational cost in a CNN structure involving high-dimensional and complex hyperparameters. In the study of [13], a Bayesian optimization technique based on the Gaussian process was used to optimize hyperparameters such as the learning rate of CNN, the epoch, and the initial weight of the convolution and fully-connected layers. Many of the hyperparameters in this study have continuous values ​​and are affected by the normalization of the data, but have little correlation with the structure of the CNN itself. [17, 18, 19] also conducted optimization studies on the continuous hyperparameter values ​​of deep neural networks in a similar manner, and in [19, 20, 21, 22, 23, 24] studies, unlike previous studies, Applied techniques that can automatically update hyperparameter values ​​are proposed, such as adjusting the learning rate or reducing the weight value to improve the execution speed of backpropagation. In addition, early stopping [25, 26] is sometimes used when the accuracy of the artificial neural network is no longer improved as learning progresses or the error rate increases as the epoch for learning progresses. [27] In the paper, an effective technique that can be used to initialize the weights of the convolution layer and the fully-connected layers is proposed, and the Evolutionary algorithm is the most widely used in automating the configuration of the learning algorithm structure of artificial neural networks such as CNN. It can be called an algorithm. [28] The genetic algorithm used in the study is used to optimize the filter size and number of convolution layers. It has a structure consisting of three convolution layers and one fully-connected layer, and hyperparameters such as layer depth and pooling size are optimized. The accuracy is not as high as 75% because it is not in the state. The genetic algorithm has a disadvantage that the cost for computation is quite high, because all cases (population) involved in the genetic algorithm are composed of individual CNN objects, and each object needs to be trained, corrected, and verified every time. In the study of [29], optimization studies were conducted only on fully-connected layers of artificial neural networks, but a reqularization technique is used to automatically set the number of units of the corresponding fully-connected layer. In recent years, attention has been focused on the structural design-related parts of deep learning, and [30] applied reinforcement learning and RNN to the study to search for the optimal structure of neural networks, showing quite impressive results. In [31], it was proposed to use Evolution of Augmenting Topologies (NEAT) based on CoDeepNEAT to determine the shape of each layer of neural network and its hyperparameters. In [32], a genetic algorithm is used to design a complex CNN structure, and each of the above experiments used a method of simultaneously performing more than 250 variance tasks to find the optimal structure. Reinforcement learning based on Q-learning [33] was utilized as a method to search for an optimal structure for only one layer at a time, and the depth of the corresponding layer uses a value preset by the researcher.
However, the results obtained through reinforcement learning were tasks requiring considerable system resources and long hours of effort. [34] In the study, we used a method to visualize and monitor the optimization of neural network structures, and the speed was improved by reducing the stride and kernel size of the first layer of convolution. However, the problems inherent in the CNN structure used in the study or issues in progress had the disadvantage that a researcher with specialized knowledge must directly analyze it. In addition, humans had to be directly involved in the selection and selection of a new CNN structure. Most of the existing hyperparameter optimization or neural network model selection is a method of verifying various models in order to maximize the accuracy of the validation set. [35] In the study, deconvnet was proposed, which visualizes images learned through a neural network to measure and monitor the accuracy, thereby presenting a new method to reduce the error rate of CNN. This is a method that can be automatically set through. However, there is a disadvantage in that it cannot be applied to datasets other than image data because optimization is performed through visualization of the learned image.

### Evaluation Metrics
Evaluation of CNN performance changes according to each parameter change below
#### dropout- ratio change (10 ~ 80%) and performence testing for each cases
#### batch size change (128 ~ 2048) and performence testing for each cases
#### convolutional layer change (2 ~ 6) and performence testing for each cases
#### fully connected layer change(1 ~ 5) and performence testing for each cases

### Project Design
Various hyperparameters such as the size of the kernel constituting the convolutional neural network (CNN), the number of channels, the stride, the number of convolution and max pooling layers, the number of fully connected layers, the number of epochs, the batch size, and the dropout ratio, etc. It has a decisive influence on not only determining but also extracting features. In this paper, in addition to research on performance optimization based on kernel size, number of channels and stride structure, which have been proven in existing CNN-related papers, changes in CNN structure, max pooling size, dropout ratio, and fully connected network structure in the abstraction stage of CNN. The purpose of this study was to examine how it affects the final performance of this convolutional neural network. After basic configuration of the CNN, kernel, channel, and stride hyperparameter to have a certain performance through the implementation of a simple structure, CNN structure, max pooling size, dropout ratio, and fully connected layer to be observed are set as observation variables to learn and learn. The trend and correlation of test performance changes were observed. The CNN model used in this paper was trained and tested using the CIFAR-10 Dataset.
